{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, textblob, re\n",
    "import pickle\n",
    "import nltk \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from unidecode import unidecode\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, decomposition, ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras import layers, models, optimizers\n",
    "from keras.preprocessing import text, sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/out_Tax.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde98a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "df = df.dropna()\n",
    "df = df[df[\"Text\"].str.contains(\"Duitse vertaling\")==False]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a39349",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = df[['Text']]\n",
    "type(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result\n",
    "\n",
    "def remove_slash_with_space(text): \n",
    "    return text.replace('\\\\', \" \")\n",
    "\n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower()  \n",
    "\n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = stopwords.words('dutch')\n",
    "    nstopword = ['bis', 'NL', 'FR', 'artikel','wijziging','http','kbwib', 'koninklijk','einde' ,'koninklijke', 'minister' ,'koning', 'besluit', 'belgisch',\n",
    "                 'tabel' ,'definitief', 'ministrieel', 'staatsblad', 'oktober', 'september','november','december','januari','februari',\n",
    "                 'maart', 'april','mei','juli', 'juni', 'augustus', 'begin','navigatie', 'eerste', 'woord', 'laatste', 'woord',\n",
    "                 'publicatie', 'numac','wwwejusticejustfgovbeelibesluitstaatsblad', 'belgielexbe', 'ministerie', 'ministerieel','identificatiecode','kruispuntbank', 'wetgeving', 'raad']\n",
    "    stop_words.extend(nstopword)\n",
    "    #print('++++++',stop_words)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if (len(word) >3) and word not in stop_words] \n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def stem_words(text): \n",
    "    stemmer = PorterStemmer() \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    return ' '.join(stems)\n",
    "\n",
    "def lemmatize_words(text): \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return ' '.join(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform preprocessing\n",
    "\n",
    "def perform_preprocessing(text):\n",
    "    \n",
    "    text = remove_accented_chars(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_slash_with_space(text)\n",
    "    text = remove_punctuation(text)\n",
    "    # text = stem_words(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = lemmatize_words(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "text_sample= df['Text']\n",
    "type(text_sample)\n",
    "\n",
    "df['Lema'] = df['Text'].apply(perform_preprocessing)\n",
    "print(df['Lema'][28])\n",
    "print('++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "text = lemmatize_words(df['Lema'][28])\n",
    "text = remove_stopwords(text)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lema'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ff391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()\n",
    "df = df[df.Num_words != 0]\n",
    "#df = df[df.Num_words != 37]\n",
    "\n",
    "#l = df[df['Num_words'] < 100]\n",
    "\n",
    "#findex = df['Lema'][24].index('numac')\n",
    "#sindex = len(df['Lema'][25]) - df['Lema'][25].reverse().index('begin')-1\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab970d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lema'][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ebdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "doc = df['Lema'][37]\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(doc)\n",
    "for kw in keywords:\n",
    "  print(kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "r = Rake()\n",
    "my_text = \"\"\"\n",
    "When it comes to evaluating the performance of keyword extractors, you can use some of the standard metrics in machine learning: accuracy, precision, recall, and F1 score. However, these metrics donâ€™t reflect partial matches; they only consider the perfect match between an extracted segment and the correct prediction for that tag.\n",
    "Fortunately, there are some other metrics capable of capturing partial matches. An example of this is ROUGE.\n",
    "\"\"\"\n",
    "ss = df['Lema'][48][250:3700]\n",
    "print(len(ss))\n",
    "r.extract_keywords_from_text(ss)\n",
    "keywordList           = []\n",
    "rankedList            = r.get_ranked_phrases_with_scores()\n",
    "for keyword in rankedList:\n",
    "  keyword_updated       = keyword[1].split()\n",
    "  keyword_updated_string    = \" \".join(keyword_updated[:2])\n",
    "  keywordList.append(keyword_updated_string)\n",
    "  if(len(keywordList)>9):\n",
    "    break\n",
    "print(keywordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "l = spacy.explain('PROPN')\n",
    "from spacy.lang.nl.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "def get_hotwords(text):\n",
    "    result = []\n",
    "    pos_tag = ['ADJ', 'NOUN'] \n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return ' '.join(result)\n",
    "df['Sum'] = df['Lema'].apply(get_hotwords)\n",
    "df[\"Num_words\"] = df['Lema'].apply(lambda x: len(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3779fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample = df['Text'][28]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df[['Lema']]\n",
    "dictionary = gensim.corpora.Dictionary(l)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa95ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
