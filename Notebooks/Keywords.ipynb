{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8013b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\krist\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0169f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one text\n",
    "\n",
    "def keywords(text):\n",
    "    string = text\n",
    "    '--------------------------'\n",
    "    \n",
    "    def pre_process(string):\n",
    "        # lowercase\n",
    "        string =string.lower()\n",
    "    \n",
    "        #remove tags\n",
    "        string =re.sub(\"</?.*?>\",\" <> \",text)\n",
    "    \n",
    "        # remove special characters and digits\n",
    "        string =re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "        return string\n",
    "    \n",
    "    article = pre_process(string)\n",
    "    \n",
    "    '--------------------------'\n",
    "    #load a set of stop words\n",
    "    stop_words= stopwords.words(\"dutch\")\n",
    "    stop_words.extend(['bis', 'NL', 'FR','artikel', \"januari\", \"februari\", \"maart\", \"april\", \"mei\", \"juni\", \"juli\", \"augustus\", \"september\", \"oktober\", \"november\", \"december\"])\n",
    "    \n",
    "    #get the text \n",
    "    \n",
    "    docs=[article]  \n",
    "    \n",
    "    #create a vocabulary of words, \n",
    "    #ignore words that appear in 85% of documents, \n",
    "    #eliminate stop words\n",
    "    cv=CountVectorizer(stop_words=stop_words)\n",
    "    word_count_vector=cv.fit_transform(docs)\n",
    "    \n",
    "    # transform it to tf-idf vector\n",
    "    \n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    \n",
    "    '--------------------------'\n",
    "    \n",
    "    def sort_coo(coo_matrix):\n",
    "        tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "        return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "    '--------------------------'\n",
    "    \n",
    "    def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "        \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "        #use only topn items from vector\n",
    "        sorted_items = sorted_items[:topn]\n",
    "\n",
    "        score_vals = []\n",
    "        feature_vals = []\n",
    "\n",
    "        for idx, score in sorted_items:\n",
    "            fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "            score_vals.append(round(score, 3))\n",
    "            feature_vals.append(feature_names[idx])\n",
    "\n",
    "        #create a tuples of feature,score\n",
    "        #results = zip(feature_vals,score_vals)\n",
    "        results= {}\n",
    "        for idx in range(len(feature_vals)):\n",
    "            results[feature_vals[idx]]=score_vals[idx]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    '--------------------------'\n",
    "    \n",
    "    # you only needs to do this once\n",
    "    feature_names=cv.get_feature_names()\n",
    "    \n",
    "    result_list=[]\n",
    "    def get_keywords(x):\n",
    "\n",
    "        #generate tf-idf for the given document\n",
    "        tf_idf_vector=tfidf_transformer.transform(cv.transform(x))\n",
    "\n",
    "        #sort the tf-idf vectors by descending order of scores\n",
    "        sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "        #extract only the top n; n here is 10\n",
    "        keys=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "        \n",
    "        result_list.append(list(keys.keys()))\n",
    "        return result_list\n",
    "    \n",
    "    end = get_keywords(docs)\n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a31e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a871275",
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe = keywords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e986ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_score(s1):\n",
    "# Two lists of sentences\n",
    "\n",
    "    #s1 = ['brusselse overlijden driejarige fiscale behoud begroting belast familiale gunstregime decujus kader bedoeld gaan wetboek regering vervuld bekomen gebleven periode voorwaarden financien overdracht zoals hoofdstedelijke aanving']\n",
    "    s2 = ['belasting fiscaliteit aanslagjaar inkomsten']\n",
    "\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarits\n",
    "    cosine_scores = float(util.cos_sim(embeddings1, embeddings2))\n",
    "\n",
    "    #Output the pairs with their score\n",
    "    cosine_scores = (\"{:.3f}\".format(cosine_scores))\n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f456dc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.166'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_score(maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a8d9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through a csv file\n",
    "\n",
    "def keywords_csv(filepath):\n",
    "    df_idf= pd.read_csv(filepath)\n",
    "    \n",
    "    # Removing German translations.\n",
    "    \n",
    "    df_idf= df_idf[df_idf[\"Text\"].str.contains(\"Duitse vertaling\")==False]\n",
    "    \n",
    "    def pre_process(text):\n",
    "    \n",
    "        # lowercase\n",
    "        text=text.lower()\n",
    "    \n",
    "        #remove tags\n",
    "        text=re.sub(\"</?.*?>\",\" <> \",text)\n",
    "    \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "        return text\n",
    "\n",
    "    df_idf['Text'] = df_idf['Text'].apply(lambda x:pre_process(x))\n",
    "\n",
    "    #load a set of stop words\n",
    "    \n",
    "    stop_words= stopwords.words(\"dutch\")\n",
    "    stop_words.extend(['bis', 'NL', 'FR','artikel', \"januari\", \"februari\", \"maart\", \"april\", \"mei\", \"juni\", \"juli\", \"augustus\", \"september\", \"oktober\", \"november\", \"december\"])\n",
    "\n",
    "    #get the text column \n",
    "    \n",
    "    docs=df_idf['Text'].tolist()\n",
    "    docs_title=df_idf['Title'].tolist()\n",
    "    \n",
    "    #create a vocabulary of words, \n",
    "    #ignore words that appear in 85% of documents, \n",
    "    #eliminate stop words\n",
    "    cv=CountVectorizer(max_df=0.85,stop_words=stop_words)\n",
    "    word_count_vector=cv.fit_transform(docs)\n",
    "    \n",
    "    # Call TfidTransformer\n",
    "    \n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    \n",
    "    def sort_coo(coo_matrix):\n",
    "        tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "        return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "    def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "        \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "        #use only topn items from vector\n",
    "        sorted_items = sorted_items[:topn]\n",
    "\n",
    "        score_vals = []\n",
    "        feature_vals = []\n",
    "\n",
    "        for idx, score in sorted_items:\n",
    "            fname = feature_names[idx]\n",
    "        \n",
    "            #keep track of feature name and its corresponding score\n",
    "            score_vals.append(round(score, 3))\n",
    "            feature_vals.append(feature_names[idx])\n",
    "\n",
    "        #create a tuples of feature,score\n",
    "        #results = zip(feature_vals,score_vals)\n",
    "        results= {}\n",
    "        for idx in range(len(feature_vals)):\n",
    "            results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "        return results\n",
    "    \n",
    "    # you only needs to do this once\n",
    "    feature_names=cv.get_feature_names()\n",
    "\n",
    "    # put the common code into several methods\n",
    "    def get_keywords(idx):\n",
    "\n",
    "        #generate tf-idf for the given document\n",
    "        tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
    "\n",
    "        #sort the tf-idf vectors by descending order of scores\n",
    "        sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "        #extract only the top n; n here is 10\n",
    "        keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "        return keywords\n",
    "\n",
    "       #generate tf-idf for all documents in your list.\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform(docs))\n",
    "\n",
    "    results=[]\n",
    "    for i in range(tf_idf_vector.shape[0]):\n",
    "    \n",
    "        # get vector for a single document\n",
    "        curr_vector=tf_idf_vector[i]\n",
    "    \n",
    "        #sort the tf-idf vector by descending order of scores\n",
    "        sorted_items=sort_coo(curr_vector.tocoo())\n",
    "\n",
    "        #extract only the top n; n here is 10\n",
    "        keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "        results.append(list(keywords.keys()))\n",
    "\n",
    "    df=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
    "    \n",
    "    return df.head()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41eca1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:394: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['fr', 'nl'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\krist\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nl fr belgiëlex be kruispuntbank wetgeving raa...</td>\n",
       "      <td>[familiale, successierechten, gunstregime, dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nl fr einde eerste woord laatste woord publica...</td>\n",
       "      <td>[onderwijs, wetenschappen, promotie, specialis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nl fr belgiëlex be kruispuntbank wetgeving raa...</td>\n",
       "      <td>[wib, kb, groepsbijdrage, quater, aftrek, xxvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nl fr belgiëlex be kruispuntbank wetgeving raa...</td>\n",
       "      <td>[quater, aanduiding, gedelegeerde, besluit, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nl fr einde eerste woord laatste woord publica...</td>\n",
       "      <td>[vennootschap, startende, aandelen, startersfo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc  \\\n",
       "0  nl fr belgiëlex be kruispuntbank wetgeving raa...   \n",
       "1  nl fr einde eerste woord laatste woord publica...   \n",
       "2  nl fr belgiëlex be kruispuntbank wetgeving raa...   \n",
       "3  nl fr belgiëlex be kruispuntbank wetgeving raa...   \n",
       "4  nl fr einde eerste woord laatste woord publica...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [familiale, successierechten, gunstregime, dri...  \n",
       "1  [onderwijs, wetenschappen, promotie, specialis...  \n",
       "2  [wib, kb, groepsbijdrage, quater, aftrek, xxvi...  \n",
       "3  [quater, aanduiding, gedelegeerde, besluit, in...  \n",
       "4  [vennootschap, startende, aandelen, startersfo...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_csv('KPMG Tax Case - Scraped Data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
